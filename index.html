<!doctype html>
<html lang="" class="no-js">
    <head>
        <title>Zhenyu Liu</title>
        <script type="text/x-mathjax-config">
             MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
        </script>
        <script type="text/x-mathjax-config">
            
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
        <script src="https://kit.fontawesome.com/362b40be54.js" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="assets/profile.css">
        <link rel="stylesheet" href="assets/style.css">
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-4RVZBSHDS4"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() {
                dataLayer.push(arguments);
            }
            gtag('js', new Date());

            gtag('config', 'G-4RVZBSHDS4');
        </script>
    </head>
    <body>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post">
                    <div itemscope itemtype="http://schema.org/Person">
                        <table border="0">
                            <tbody>
                                <tr>
                                    <td>
                                        <div class="author__avatar">
                                            <img src="https://chhzh123.github.io/images/profile-2025.jpeg" class="author__avatar" alt="Zhenyu Liu">
                                        </div>
                                    </td>
                                    <td>
                                        <div class="author__content">
                                            <h1 style=font-size:28px>
                                                <p class="author__name">
                                                    <b>Zhenyu Liu</b>
                                                </p>
                                            </h1>
                                            <ul class="contact-list">
                                                <li>Ph.D. Student</li>
                                                <li>
                                                    <a href="https://ecse.rpi.edu/">Department of Electrical, Computer, and Systems Engineering</a>
                                                </li>
                                                <li>
                                                    <a href="https://www.rpi.edu/">Rensselaer Polytechnic Institute</a>
                                                </li>
                                            </ul>
                                            <!-- <p style="display:inline-table;">&nbsp;<i class="far fa-building" aria-hidden="true"></i> Office:  -->
                                            <table border="0" width="80%" cellspacing="0" style="margin-top:-10px; margin-bottom:0px; margin-left:0px; margin-right:0px; display:inline-table; border-collapse: collapse;">
                                                <tr>
                                                    <td rowspan="2" width="15%" valign="middle" style="padding: 0px 0px 15px 0px;">
                                                        &nbsp;<i class="far fa-building" aria-hidden="true"></i>
                                                        Office: CII 6116, 110 8th St, Troy, NY, 12180</td>
                                                </tr>
                                            </table>
                                            <p>
                                                <i class="fas fa-fw fa-envelope" aria-hidden="true"></i>
                                                Email: liuz32 [at] rpi [dot] edu
                                                &nbsp;
                                                <a href="https://github.com/Zhenyu001225">
                                                    <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                                                    Github
                                                </a>
                                                /
                      
                                                <a href="https://scholar.google.com/citations?user=nCvMoVMAAAAJ&hl=zh-CN">
                                                    <i class="fas fa-fw fa-graduation-cap"></i>
                                                    Google Scholar
                                                </a>
                                                /
                      
                                                <a href="https://www.linkedin.com/in/zhenyu-liu-85686828a/">
                                                    <i class="fab fa-fw fa-linkedin"></i>
                                                    LinkedIn
                                                </a>
                                            </p>
                                        </div>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="post-content">
                        <p>
                            I am a Ph.D. student in the <a href="https://liuliu-cs.github.io/group/">EPIC Lab</a>
                            at <a href="https://www.rpi.edu/">Rensselaer Polytechnic Institute</a>
                            , advised by Prof. <a href="https://liuliu-cs.github.io/index.html">Liu Liu</a>
                            .
I received my B.E. in Software Engineering from the <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China</a>.
                        </p>
                        <h3 id="research-highlights">Research Highlights</h3>
                        <!-- <div style="border-left: 4px solid #0066cc; padding-left: 15px; margin: 20px 0;">
                            <p>
                                My research interests lie in <strong>machine learning systems</strong>,
                                <strong>software-hardware co-design</strong>, and <strong>trustworthy AI</strong>.
                                I focus on building <em>robust and efficient</em> foundations for both
                                vision models and large language models, including <strong>system-level optimization</strong>,
                                <strong>architecture support</strong>, and <strong>robustness/safety alignment techniques</strong>.
                            </p>
                        </div>
                        
                        <div style="border: 2px solidrgb(234, 234, 234); padding: 20px; border-radius: 8px; background-color: #f8f8f8; margin: 20px 0;">
                            <p>
                                <span style="color: #666; font-size: 0.9em;">
                                    <sup>‚òÖ</sup>
                                    indicates projects where I am the project lead
                                </span>
                            </p>
                            <p>
                                <span style="color: #0066cc;">
                                    <strong>Accelerator Programming Frameworks:</strong>
                                </span>
                            </p>
                            <ul>
                                <li>
                                    <a href="https://arxiv.org/abs/2510.14719">Tawa [CGO‚Äô26]</a>
                                    <sup style="color: #0066cc;">‚òÖ</sup>
                                    first introduces automatic warp specialization to generate efficient LLM kernels such as FlashAttention-3/4 on NVIDIA Hopper and Blackwell GPUs. The proposed <a href="https://github.com/triton-lang/triton/pull/6288">NVWS dialect</a>
                                    has been merged upstream into OpenAI <a href="https://github.com/triton-lang/triton/pull/6410">Triton</a>
                                    .
                                </li>
                                <li>
                                    <a href="https://github.com/awslabs/slapo">Slapo [ASPLOS‚Äô24]</a>
                                    <sup style="color: #0066cc;">‚òÖ</sup>
                                    is a distributed LLM training framework deployed at AWS, designed to balance usability and performance. It has influenced the design of ByteDance‚Äôs <a href="https://github.com/volcengine/veScale">veScale</a>
                                    and Meta‚Äôs <a href="https://github.com/pytorch/torchtitan">TorchTitan</a>
                                    .
                                </li>
                                <li>
                                    <a href="https://www.usenix.org/conference/nsdi23/presentation/liu-tianfeng">BGL [NSDI‚Äô23]</a>
                                    is a production-scale GNN training framework used at ByteDance, reducing billion-node graph training time from weeks to days.
                                </li>
                            </ul>
                            <p>
                                <span style="color: #006633;">
                                    <strong>Accelerator Design Languages:</strong>
                                </span>
                                <a href="https://github.com/cornell-zhang/allo">Allo [PLDI‚Äô24]</a>
                                <sup style="color: #006633;">‚òÖ</sup>
                                / <a href="https://arxiv.org/abs/2509.06794">Dato [arXiv‚Äô25]</a>
                                <sup style="color: #006633;">‚òÖ</sup>
                                is a Python/MLIR-based programming language for efficient ML accelerator design. It is adopted by 
                                <span class="tooltip">
                                    10+ universities and companies<span class="tooltiptext">Cornell, UCLA, UIUC, Brown, UofT, UVA, UofChiago, UIC, Imperial, Tsinghua, SJTU, Intel, AMD, Microsoft</span>
                                </span>
                                . Allo integrates multiple supporting tools, including the 
                                <a href="https://dl.acm.org/doi/10.1145/3626202.3637563">
                                    PEQC [FPGA‚Äô24, <span style="color: #cc0000;">üèÜ Best Paper Award</span>
                                    ]
                                </a>
                                equivalence checker, the <a href="https://dl.acm.org/doi/abs/10.1145/3490422.3502369">HeteroFlow [FPGA‚Äô22]</a>
                                dataflow programming framework, the 
                                <a href="https://dl.acm.org/doi/10.1145/3706628.3708870">
                                    ARIES [FPGA‚Äô25, <span style="color: #cc0000;">üéóÔ∏è Best Paper Nominee</span>
                                    ]
                                </a>
                                backend for AMD AI Engine / NPUs.
                            </p>
                            <p>
                                <span style="color: #cc6600;">
                                    <strong>Accelerator Architectures for ML:</strong>
                                </span>
                            </p>
                            <ul>
                                <li>
                                    <a href="https://dl.acm.org/doi/10.1145/3656177">LLM-FPGA [FCCM‚Äô24]</a>
                                    <sup style="color: #cc6600;">‚òÖ</sup>
                                    pioneers FPGA-based dataflow accelerator design for LLMs.
                                </li>
                                <li>
                                    <a href="https://dl.acm.org/doi/10.1145/3431920.3439296">
                                        FracBNN [FPGA‚Äô21, <span style="color: #cc0000;">üéóÔ∏è Best Paper Nominee</span>
                                        ]
                                    </a>
                                    is an efficient FPGA accelerator for quantized CNNs.
                                </li>
                            </ul>
                            <p>
                                <span style="color: #660099;">
                                    <strong>ML for Systems:</strong>
                                </span>
                            </p>
                            <ul>
                                <li>
                                    <a href="https://llvm.swoogo.com/2025devmtg/agenda">Magellan</a>
                                    <sup style="color: #660099;">‚òÖ</sup>
                                    is an LLM agentic framework that leverages <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a>
                                    to discover novel compiler optimization heuristics in LLVM and XLA.
                                </li>
                                <li>
                                    <a href="https://github.com/cornell-zhang/heurigym">HeuriGym [NeurIPS‚Äô25 Math-AI]</a>
                                    <sup style="color: #660099;">‚òÖ</sup>
                                    is an agentic benchmarking framework for evaluating LLMs‚Äô reasoning capabilities for scientific and engineering optimization problems.
                                </li>
                                <li>
                                    <a href="https://ieeexplore.ieee.org/document/8942126">DRL-Scheduler [ICCAD‚Äô19]</a>
                                    <sup style="color: #660099;">‚òÖ</sup>
                                    pioneers the use of deep reinforcement learning in EDA.
                                </li>
                            </ul>
                        </div> -->

                        
                        <h3 id="news">News</h3>
                        <div style="height: 400px; overflow-y: auto; padding-right: 10px; margin-bottom: 20px;">
                            <ul>
                                <li>
                                    <span>
                                        [11/20/25] <font color="red">[Paper]</font>
                                        Our paper on <a href="https://arxiv.org/abs/2510.14719">automatic warp specialization for NVIDIA GPUs</a>
                                        has been accepted to <a href="https://2026.cgo.org/">CGO‚Äô26</a>
                                        ! Congrats to all the coauthors!
                                    </span>
                                </li>
                                <li>
                                    <span>
                                        [11/18/25] <font color="grey">[Service]</font>
                                        Served as an external reviewer of <a href="https://mlsys.org/Conferences/2026">MLSys‚Äô26</a>
                                        .
                                    </span>
                                </li>
                            </ul>
                        </div>
                        <h3 id="publications">Publications</h3>
                        <p>
                            <strong>
                                <a href="https://arxiv.org/abs/2510.14719">Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        CGO<span class="tooltiptext">IEEE/ACM International Symposium on Code Generation and Optimization</span>
                                    </span>
                                </i>
                                , 2026
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('tawa-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('tawa-bib')">[bib]</a>
                            |

                            <a href="https://github.com/triton-lang/triton/tree/aref_auto_ws">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="tawa-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1x speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2x speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.
</div>
                        <div id="tawa-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2025tawa,
    title={Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References}, 
    author={Hongzheng Chen and Bin Fan and Alexander Collins and Bastian Hagedorn and Evghenii Gaburov and Masahiro Masuda and Matthew Brookhart and Chris Sullivan and Jason Knight and Zhiru Zhang and Vinod Grover},
    journal={arXiv preprint arXiv:2510.14719},
    year={2025}
}
</div>
                        <!-- @inproceedings{chen2026tawa,
    title={Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References}, 
    author={Hongzheng Chen and Bin Fan and Alexander Collins and Bastian Hagedorn and Evghenii Gaburov and Masahiro Masuda and Matthew Brookhart and Chris Sullivan and Jason Knight and Zhiru Zhang and Vinod Grover},
    booktitle={IEEE/ACM International Symposium on Code Generation and Optimization},
    year={2026}
} -->
                        <p>
                            <strong>
                                <a href="https://arxiv.org/abs/2509.05451">Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            Niansong Zhang, Wenbo Zhu, Courtney Golden, Dan Ilan, <strong>Hongzheng Chen</strong>
                            , Christopher Batten, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        MICRO<span class="tooltiptext">The International Symposium on Microarchitecture</span>
                                    </span>
                                </i>
                                , 2025
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('pim-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('pim-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/apu-micro25-artifact">
                                <i class="fa fa-file-archive-o" aria-hidden="true"></i>
                            </a>
                            | <a href="https://economictimes.indiatimes.com/news/international/us/gsit-stock-skyrockets-200-today-as-cornell-study-confirms-companys-apu-matches-gpu-performance/articleshow/124707644.cms">News</a>
                        </p>
                        <div id="pim-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            Compute-in-SRAM architectures offer a promising approach to achieving higher performance and energy efficiency across a range of data-intensive applications. However, prior evaluations have largely relied on simulators or small prototypes, limiting the understanding of their real-world potential. In this work, we present a comprehensive performance and energy characterization of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads. We compare the GSI APU against established architectures, including CPUs and GPUs, to quantify its energy efficiency and performance potential. We introduce an analytical framework for general-purpose compute-in-SRAM device that reveals fundamental optimization principles by modeling performance trade-offs, thereby guiding program optimizations.<br/>
                            <br/>Exploiting the fine-grained parallelism of tightly integrated memory-compute architectures requires careful data management. We address this by proposing three optimizations: communication-aware reduction mapping, coalesced DMA, and broadcast-friendly data layouts. When applied to retrieval-augmented generation (RAG) over large corpora (10GB--200GB), these optimizations enable our compute-in-SRAM system to accelerate retrieval by 5.4x-7.5x, improving end-to-end RAG latency by 1.1x-1.8x. The shared off-chip memory bandwidth is modeled using a simulated HBM, while all other components are measured on the real compute-in-SRAM device. Critically, this system matches the performance of an NVIDIA A6000 GPU for RAG while being significantly more energy-efficient (46.1x-107.5x reduction). These findings validate the viability of compute-in-SRAM for complex, real-world applications and provide guidance for advancing the technology.

                        </div>
                        <div id="pim-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{zhang2025pim,
  title={Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device},
  author={Niansong Zhang and Wenbo Zhu and Courtney Golden and Dan Ilan and Hongzheng Chen and Christopher Batten and Zhiru Zhang},
  booktitle = {Proceedings of the 58th Annual IEEE/ACM International Symposium on Microarchitecture},
  year = {2025}
}
</div>
                        <!-- https://ir.gsitechnology.com/news-releases/news-release-details/compute-memory-apu-achieves-gpu-class-ai-performance-fraction -->
                        <!-- https://economictimes.indiatimes.com/news/international/us/gsit-stock-skyrockets-200-today-as-cornell-study-confirms-companys-apu-matches-gpu-performance/articleshow/124707644.cms?from=mdr -->
                        <!-- https://www.stocktitan.net/news/GSIT/compute-in-memory-apu-achieves-gpu-class-ai-performance-at-a-a9souzbf11hy.html -->
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3706628.3708870">üéóÔ∏è ARIES: An Agile MLIR-Based Compilation Flow for Reconfigurable Devices with AI Engines</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            Jinming Zhuang*, Shaojie Xiang*, <strong>Hongzheng Chen</strong>
                            , Niansong Zhang, Zhuoping Yang, Tony Mao, Zhiru Zhang, Peipei Zhou<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        FPGA<span class="tooltiptext">ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</span>
                                    </span>
                                </i>
                                , 2025 
                                <font color="orange">
                                    <b>(Best Paper Nominee)</b>
                                </font>
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('aries-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('aries-bib')">[bib]</a>
                            |

                            <a href="https://github.com/arc-research-lab/Aries">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="aries-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            As AI continues to grow, modern applications are becoming more data- and compute-intensive, driving the development of specialized AI chips to meet these demands. One example is AMD's AI Engine (AIE), a dedicated hardware system that includes a 2D array of high-frequency very-long instruction words (VLIW) vector processors to provide high computational throughput and reconfigurability. However, AIE's specialized architecture presents tremendous challenges in programming and compiler optimization. Existing AIE programming frameworks lack a clean abstraction to represent multi-level parallelism in AIE; programmers have to figure out the parallelism within a kernel, manually do the partition, and assign sub-tasks to different AIE cores to exploit parallelism. These significantly lower the programming productivity. Furthermore, some AIE architectures include FPGAs to provide extra flexibility, but there is no unified intermediate representation (IR) that captures these architectural differences. As a result, existing compilers can only optimize the AIE portions of the code, overlooking potential FPGA bottlenecks and leading to suboptimal performance.
<br/>
                            <br/>
                            To address these limitations, we introduce ARIES, an agile multi-level intermediate representation (MLIR) based compilation flow for reconfigurable devices with AIEs. ARIES introduces a novel programming model that allows users to map kernels to separate AIE cores, exploiting task- and tile-level parallelism without restructuring code. It also includes a declarative scheduling interface to explore instruction-level parallelism within each core. At the IR level, we propose a unified MLIR-based representation for AIE architectures, both with or without FPGA, facilitating holistic optimization and better portability across AIE device families. For the General Matrix Multiply (GEMM) benchmark, ARIES achieves 4.92 TFLOPS, 15.86 TOPS, and 45.94 TOPS throughput under FP32, INT16, and, INT8 data types on Versal VCK190 respectively. Compared with the state-of-the-art (SOTA) work CHARM for AIE, ARIES improves the throughput by 1.17x, 1.59x, and 1.47x correspondingly. For ResNet residual layer, ARIES achieves up to 22.58x speedup compared with optimized SOTA work Riallto on Ryzen-AI NPU. ARIES is open-sourced on GitHub: <a href="https://github.com/arc-research-lab/Aries">https://github.com/arc-research-lab/Aries</a>
                            .

                        </div>
                        <div id="aries-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{zhuang2025aries,
  title={ARIES: An Agile MLIR-Based Compilation Flow for Reconfigurable Devices with AI Engines},
  author={Zhuang, Jinming and Xiang, Shaojie and Chen, Hongzheng and Zhang, Niansong and Yang, Zhuoping and Mao, Tony and Zhang, Zhiru and Zhou, Peipei},
  booktitle={Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={92--102},
  year={2025}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3656401">Allo: A Programming Model for Composable Accelerator Design</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reusable">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            *, Niansong Zhang*, Shaojie Xiang, Zhichen Zeng, Mengjia Dai, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        PLDI<span class="tooltiptext">ACM SIGPLAN Conference on Programming Language Design and Implementation</span>
                                    </span>
                                </i>
                                , 2024
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('allo-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('allo-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/allo">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                            |

                            <a href="https://github.com/cornell-zhang/allo-pldi24-artifact">
                                <i class="fa fa-file-archive-o" aria-hidden="true"></i>
                            </a>
                            |
<a href="https://zhuanlan.zhihu.com/p/700829792">Blog (Zhihu)</a>
                        </p>
                        <div id="allo-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel. Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened. In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives. Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner. This approach facilitates holistic optimizations that span across function boundaries. We conduct comprehensive experiments on commonly-used HLS benchmarks and several realistic deep learning models. Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench. For the GPT2 model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs.
</div>
                        <div id="allo-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2024allo,
  title={Allo: A programming model for composable accelerator design},
  author={Chen, Hongzheng and Zhang, Niansong and Xiang, Shaojie and Zeng, Zhichen and Dai, Mengjia and Zhang, Zhiru},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={PLDI},
  pages={593--620},
  year={2024},
  publisher={ACM New York, NY, USA}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3656177">Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao Yue, Niansong Zhang, Yaohui Cai, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        ACM TRETS<span class="tooltiptext">ACM Transactions on Reconfigurable Technology and Systems</span>
                                    </span>
                                </i>
                                , 2024 (
                                <span class="tooltip">
                                    FCCM<span class="tooltiptext">IEEE International Symposium on Field-Programmable Custom Computing Machines</span>
                                </span>
                                ‚Äò24 Journal Track)
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('llm-fpga-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('llm-fpga-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/allo/tree/main/examples">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                            |
<a href="https://zhuanlan.zhihu.com/p/674607019">Blog (Zhihu)</a>
                        </p>
                        <div id="llm-fpga-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. While hardware accelerators for Transformer-based models have been extensively studied, the majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead.<br/>
                            <br/>
                            This article investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on field-programmable gate arrays (FPGAs). Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. This model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can identify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart.<br/>
                            <br/>To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT2) on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4√ó speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2√ó speedup compared to Design for Excellence, an FPGA overlay, in the prefill stage, while achieving a 1.9√ó speedup and a 5.7√ó improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.

                        </div>
                        <div id="llm-fpga-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2024understanding,
  title={Understanding the potential of fpga-based spatial acceleration for large language model inference},
  author={Chen, Hongzheng and Zhang, Jiahao and Du, Yixiao and Xiang, Shaojie and Yue, Zichao and Zhang, Niansong and Cai, Yaohui and Zhang, Zhiru},
  journal={ACM Transactions on Reconfigurable Technology and Systems},
  volume={18},
  number={1},
  pages={1--29},
  year={2024},
  publisher={ACM New York, NY}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3620665.3640399">Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Cody Hao Yu, Shuai Zheng, Zhen Zhang, Zhiru Zhang, Yida Wang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        ASPLOS<span class="tooltiptext">ACM International Conference on Architectural Support for Programming Languages and Operating Systems</span>
                                    </span>
                                </i>
                                , 2024
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('slapo-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('slapo-bib')">[bib]</a>
                            |

                            <a href="https://github.com/awslabs/slapo">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                            |

                            <a href="https://github.com/chhzh123/slapo-artifact">
                                <i class="fa fa-file-archive-o" aria-hidden="true"></i>
                            </a>
                            |
<a href="https://www.amazon.science/publications/slapo-a-schedule-language-for-progressive-optimization-of-large-deep-learning-model-training">Amazon Science</a>
                        </p>
                        <div id="slapo-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            Recent years have seen an increase in the development of large deep learning (DL) models, which makes training efficiency crucial. Common practice is struggling with the trade-off between usability and performance. On one hand, DL frameworks such as PyTorch use dynamic graphs to facilitate model developers at a price of sub-optimal model training performance. On the other hand, practitioners propose various approaches to improving the training efficiency by sacrificing some of the flexibility, ranging from making the graph static for more thorough optimization (e.g., XLA) to customizing optimization towards large-scale distributed training (e.g., DeepSpeed and Megatron-LM).<br/>
                            <br/>In this paper, we aim to address the tension between usability and training efficiency through separation of concerns. Inspired by DL compilers that decouple the platform-specific optimizations of a tensor-level operator from its arithmetic definition, this paper proposes a schedule language, Slapo, to decouple model execution from definition. Specifically, Slapo works on a PyTorch model and uses a set of schedule primitives to convert the model for common model training optimizations such as high-performance kernels, effective 3D parallelism, and efficient activation checkpointing. Compared to existing optimization solutions, Slapo progressively optimizes the model "as-needed" through high-level primitives, and thus preserving programmability and debuggability for users to a large extent. Our evaluation results show that by scheduling the existing hand-crafted optimizations in a systematic way using Slapo, we are able to improve training throughput by up to 2.92√ó on a single machine with 8 NVIDIA V100 GPUs, and by up to 1.41√ó on multiple machines with up to 64 GPUs, when compared to the out-of-the-box performance of DeepSpeed and Megatron-LM.

                        </div>
                        <div id="slapo-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{chen2024slapo,
  title={Slapo: A schedule language for progressive optimization of large deep learning model training},
  author={Chen, Hongzheng and Yu, Cody Hao and Zheng, Shuai and Zhang, Zhen and Zhang, Zhiru and Wang, Yida},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={1095--1111},
  year={2024}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3626202.3637563">üèÜ Formal Verification of Source-to-Source Transformations for HLS</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            Louis-No√´l Pouchet, Emily Tucker, Niansong Zhang, <strong>Hongzheng Chen</strong>
                            , Debjit Pal, Gabriel Rodr√≠guez, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        FPGA<span class="tooltiptext">ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</span>
                                    </span>
                                </i>
                                , 2024 
                                <font color="red">
                                    <b>(Best Paper Award)</b>
                                </font>
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('hls-verify-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('hls-verify-bib')">[bib]</a>
                            |

                            <a href="https://sourceforge.net/projects/pocc/files/1.6/testing/modules/past-0.7.2.tar.gz/download">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="hls-verify-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">High-level synthesis (HLS) can greatly facilitate the description of complex hardware implementations, by raising the level of abstraction up to a classical imperative language such as C/C++, usually augmented with vendor-specific pragmas and APIs. Despite productivity improvements, attaining high performance for the final design remains a challenge, and higher-level tools like source-to-source compilers have been developed to generate programs targeting HLS toolchains. These tools may generate highly complex HLS-ready C/C++ code, reducing the programming effort and enabling critical optimizations. However, whether these HLS-friendly programs are produced by a human or a tool, validating their correctness or exposing bugs otherwise remains a fundamental challenge. In this work we target the problem of efficiently checking the semantics equivalence between two programs written in C/C++ as a means to ensuring the correctness of the description provided to the HLS toolchain, by proving an optimized code version fully preserves the semantics of the unoptimized one. We introduce a novel formal verification approach that combines concrete and abstract interpretation with a hybrid symbolic analysis. Notably, our approach is mostly agnostic to how control-flow, data storage, and dataflow are implemented in the two programs. It can prove equivalence under complex bufferization and loop/syntax transformations, for a rich class of programs with statically interpretable control-flow. We present our techniques and their complete end-to-end implementation, demonstrating how our system can verify the correctness of highly complex programs generated by source-to-source compilers for HLS, and detect bugs that may elude co-simulation.
</div>
                        <div id="hls-verify-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{pouchet2024formal,
  title={Formal verification of source-to-source transformations for hls},
  author={Pouchet, Louis-No{\"e}l and Tucker, Emily and Zhang, Niansong and Chen, Hongzheng and Pal, Debjit and Rodr{\'\i}guez, Gabriel and Zhang, Zhiru},
  booktitle={Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={97--107},
  year={2024}
}
</div>
                        <p>
                            <strong>
                                <a href="https://www.usenix.org/conference/nsdi23/presentation/liu-tianfeng">BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing</a>
                            </strong>
                            <br/>
                            Tianfeng Liu*, Yangrui Chen*, Dan Li, Chuan Wu, Yibo Zhu, Jun He, Yanghua Peng, <strong>Hongzheng Chen</strong>
                            , Hongzhi Chen, Chuanxiong Guo<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        NSDI<span class="tooltiptext">USENIX Symposium on Networked Systems Design and Implementation</span>
                                    </span>
                                </i>
                                , 2023
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('bgl-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('bgl-bib')">[bib]</a>
                            |

                            <a href="https://github.com/leodestiny/BGL_NSDI2023">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="bgl-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">Graph neural networks (GNNs) have extended the success of deep neural networks (DNNs) to non-Euclidean graph data, achieving ground-breaking performance on various tasks such as node classification and graph property prediction. Nonetheless, existing systems are inefficient to train large graphs with billions of nodes and edges with GPUs. The main bottlenecks are the process of preparing data for GPUs‚Äìsubgraph sampling and feature retrieving. This paper proposes BGL, a distributed GNN training system designed to address the bottlenecks with a few key ideas. First, we propose a dynamic cache engine to minimize feature retrieving traffic. By co-designing caching policy and the order of sampling, we find a sweet spot of low overhead and a high cache hit ratio. Second, we improve the graph partition algorithm to reduce cross-partition communication during subgraph sampling. Finally, careful resource isolation reduces contention between different data preprocessing stages. Extensive experiments on various GNN models and large graph datasets show that BGL significantly outperforms existing GNN training systems by 1.9 x on average.
</div>
                        <div id="bgl-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{liu2023bgl,
  title={BGL: GPU-EfficientGNN training by optimizing graph data I/O and preprocessing},
  author={Liu, Tianfeng and Chen, Yangrui and Li, Dan and Wu, Chuan and Zhu, Yibo and He, Jun and Peng, Yanghua and Chen, Hongzheng and Chen, Hongzhi and Guo, Chuanxiong},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={103--118},
  year={2023}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3489517.3530681">Accelerator Design with Decoupled Hardware Customizations: Benefits and Challenges</a>
                            </strong>
                            <br/>
                            Debjit Pal, Yi-Hsiang Lai, Shaojie Xiang, Niansong Zhang, <strong>Hongzheng Chen</strong>
                            , Jeremy Casas, Pasquale Cocchini, Zhenkun Yang, Jin Yang, Louis-No√´l Pouchet, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        DAC<span class="tooltiptext">ACM/IEEE Design Automation Conference</span>
                                    </span>
                                </i>
                                , 2022 (Invited Paper)
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('decoupled-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('decoupled-bib')">[bib]</a>
                        </p>
                        <div id="decoupled-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            The past decade has witnessed increasing adoption of high-level synthesis (HLS) to implement specialized hardware accelerators targeting either FPGAs or ASICs. However, current HLS programming models entangle algorithm specifications with hardware customization techniques, which lowers both the productivity and portability of the accelerator design. To tackle this problem, recent efforts such as HeteroCL propose to decouple algorithm definition from essential hardware customization techniques in compute, data type, and memory, increasing productivity, portability, and performance.<br/>
                            <br/>While the decoupling of the algorithm and customizations provides benefits to the compilation/synthesis process, they also create new hurdles for the programmers to productively debug and validate the correctness of the optimized design. In this work, using HeteroCL and realistic machine learning applications as case studies, we first explain the key advantages of the decoupled programming model brought to a programmer to rapidly develop high-performance accelerators. Using the same case studies, we will further show how seemingly benign usage of the customization primitives can lead to new challenges to verification. We will then outline the research opportunities and discuss some of our recent efforts as the first step to enable a robust and viable verification solution in the future.

                        </div>
                        <div id="decoupled-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{pal2022accelerator,
  title={Accelerator design with decoupled hardware customizations: benefits and challenges},
  author={Pal, Debjit and Lai, Yi-Hsiang and Xiang, Shaojie and Zhang, Niansong and Chen, Hongzheng and Casas, Jeremy and Cocchini, Pasquale and Yang, Zhenkun and Yang, Jin and Pouchet, Louis-No{\"e}l and others},
  booktitle={Proceedings of the 59th ACM/IEEE Design Automation Conference},
  pages={1351--1354},
  year={2022}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/abs/10.1145/3490422.3502369">HeteroFlow: An Accelerator Programming Model with Decoupled Data Placement for Software-Defined FPGAs</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            Shaojie Xiang, Yi-Hsiang Lai, Yuan Zhou, <strong>Hongzheng Chen</strong>
                            , Niansong Zhang, Debjit Pal, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        FPGA<span class="tooltiptext">ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</span>
                                    </span>
                                </i>
                                , 2022
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('heteroflow-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('heteroflow-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/heterocl">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="heteroflow-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">To achieve high performance with FPGA-equipped heterogeneous compute systems, it is crucial to co-optimize data placement and compute scheduling to maximize data reuse and bandwidth utilization for both on- and off-chip memory accesses. However, optimizing the data placement for FPGA accelerators is a complex task. One must acquire in-depth knowledge of the target FPGA device and its associated memory system in order to apply a set of advanced optimizations. Even with the latest high-level synthesis (HLS) tools, programmers often have to insert many low-level vendor-specific pragmas and substantially restructure the algorithmic code so that the right data are accessed at the right loop level using the right communication schemes. These code changes can significantly compromise the composability and portability of the original program. To address these challenges, we propose HeteroFlow, an FPGA accelerator programming model that decouples the algorithm specification from optimizations related to orchestrating the placement of data across a customized memory hierarchy. Specifically, we introduce a new primitive named .to(), which provides a unified programming interface for specifying data placement optimizations at different levels of granularity: (1) coarse-grained data placement between host and accelerator, (2) medium-grained kernel-level data placement within an accelerator, and (3) fine-grained data placement within a kernel. We build HeteroFlow on top of the open-source HeteroCL DSL and compilation framework. Experimental results on a set of realistic benchmarks show that, programs written in HeteroFlow can match the performance of extensively optimized manual HLS design with much fewer lines of code.
</div>
                        <div id="heteroflow-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{xiang2022heteroflow,
  title={Heteroflow: An accelerator programming model with decoupled data placement for software-defined fpgas},
  author={Xiang, Shaojie and Lai, Yi-Hsiang and Zhou, Yuan and Chen, Hongzheng and Zhang, Niansong and Pal, Debjit and Zhang, Zhiru},
  booktitle={Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={78--88},
  year={2022}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3458817.3476159">Krill: A Compiler and Runtime System for Concurrent Graph Processing</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Minghua Shen, Nong Xiao, Yutong Lu<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        SC<span class="tooltiptext">International Conference for High Performance Computing, Networking, Storage and Analysis</span>
                                    </span>
                                </i>
                                , 2021
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('krill-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('krill-bib')">[bib]</a>
                            |

                            <a href="https://github.com/chhzh123/Krill">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                            |

                            <a href="https://zenodo.org/records/5165762">
                                <i class="fa fa-file-archive-o" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="krill-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">As a large number of emerging graph applications spread across different domains, the need for processing massive concurrent graph jobs (CGJs) is increasing. However, existing graph processing systems designed for a single job cannot efficiently tackle multiple CGJs, where they suffer from interfering memory access patterns and inefficient property management. In this paper, we introduce Krill, a compiler and runtime system for processing concurrent graph jobs. We propose an SAP model, which decouples graph structure, algorithm, and property. In the compiler, we propose leveraging the property buffer to easily write and manage property data. In the runtime system, we propose a novel technique named graph kernel fusion to reduce memory accesses, which fuses all the jobs and processes them as a whole. Experimental results show our system significantly reduces the number of memory accesses for CGJs by more than 6x compared with the baseline, and achieves up to 6.76x speedup with 3.84x shorter response latency than GraphM, the state-of-the-art concurrent graph processing system.
</div>
                        <div id="krill-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{chen2021krill,
  title={Krill: a compiler and runtime system for concurrent graph processing},
  author={Chen, Hongzheng and Shen, Minghua and Xiao, Nong and Lu, Yutong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2021}
}
</div>
                        <p>
                            <strong>
                                <a href="https://dl.acm.org/doi/10.1145/3431920.3439296">üéóÔ∏è FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations</a>
                            </strong>
                            <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
                                <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" style="vertical-align:text-top;" width="2.5%"/>
                            </a>
                            <br/>
                            Yichi Zhang, Junhao Pan, Xinheng Liu, <strong>Hongzheng Chen</strong>
                            , Deming Chen, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        FPGA<span class="tooltiptext">ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</span>
                                    </span>
                                </i>
                                , 2021 
                                <font color="orange">
                                    <b>(Best Paper Nominee)</b>
                                </font>
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('fracbnn-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('fracbnn-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/FracBNN">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="fracbnn-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            Binary neural networks (BNNs) have 1-bit weights and activations. Such networks are well suited for FPGAs, as their dominant computations are bitwise arithmetic and the memory requirement is also significantly reduced. However, compared to start-of-the-art compact convolutional neural network (CNN) models, BNNs tend to produce a much lower accuracy on realistic datasets such as ImageNet. In addition, the input layer of BNNs has gradually become a major compute bottleneck, because it is conventionally excluded from binarization to avoid a large accuracy loss.<br/>
                            <br/>This work proposes FracBNN, which exploits fractional activations to substantially improve the accuracy of BNNs. Specifically, our approach employs a dual-precision activation scheme to compute features with up to two bits, using an additional sparse binary convolution. We further binarize the input layer using a novel thermometer encoding. Overall, FracBNN preserves the key benefits of conventional BNNs, where all convolutional layers are computed in pure binary MAC operations (BMACs). We design an efficient FPGA-based accelerator for our novel BNN model that supports the fractional activations. To evaluate the performance of FracBNN under a resource-constrained scenario, we implement the entire optimized network architecture on an embedded FPGA (Xilinx Ultra96 v2). Our experiments on ImageNet show that FracBNN achieves an accuracy comparable to MobileNetV2, surpassing the best-known BNN design on FPGAs with an increase of 28.9% in top-1 accuracy and a 2.5x reduction in model size. FracBNN also outperforms a recently introduced BNN model with an increase of 2.4% in top-1 accuracy while using the same model size. On the embedded FPGA device, FracBNN demonstrates the ability of real-time image classification.

                        </div>
                        <div id="fracbnn-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{zhang2021fracbnn,
  title={FracBNN: Accurate and FPGA-efficient binary neural networks with fractional activations},
  author={Zhang, Yichi and Pan, Junhao and Liu, Xinheng and Chen, Hongzheng and Chen, Deming and Zhang, Zhiru},
  booktitle={The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={171--182},
  year={2021}
}
</div>
                        <p>
                            <strong>
                                <a href="https://ieeexplore.ieee.org/document/8823964">Entropy-Directed Scheduling for FPGA High-Level Synthesis</a>
                            </strong>
                            <br/>
                            Minghua Shen, <strong>Hongzheng Chen</strong>
                            *, Nong Xiao<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        IEEE TCAD<span class="tooltiptext">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</span>
                                    </span>
                                </i>
                                , 2020
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('entropy-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('entropy-bib')">[bib]</a>
                            |

                            <a href="https://github.com/chhzh123/Entropy-directed-scheduling">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="entropy-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">High-level synthesis (HLS) is important for compiling an application design onto field-programmable gate array (FPGA) but still faces challenges of balancing scalability and quality of results in the scheduling process. In this article, we propose an entropy-directed scheduling (EDS) algorithm that efficiently generates high-quality schedules for FPGA HLS. This article is novel in three ways. First, we make the first attempt to adopt entropy in the scheduling of HLS, which is an intuitive and robust measurement with lots of good analytic properties. Second, we creatively leverage the maximum entropy principle to describe the scheduling process, which is proved equivalent to the optimal solution in some particular cases. Third, we make EDS automatically analyze the input graph structure and leverage a three-stage scheduling process to obtain high-quality results. As a result, EDS has the lowest time complexity among existing scheduling algorithms and is flexible to solve both latency- and resource-constrained problems while satisfying other constraints. The experimental results show that for latency-constrained scheduling problem, EDS reduces up to 68% resource usage and is 294√ó faster than the force-directed scheduling algorithm. For resource-constrained scheduling problem, EDS obtains near-optimal solutions with an average speedup of 16 410√ó compared with ILP. To our best knowledge, this is the first EDS algorithm for FPGA HLS.
</div>
                        <div id="entropy-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{shen2019entropy,
  title={Entropy-directed scheduling for FPGA high-level synthesis},
  author={Shen, Minghua and Chen, Hongzheng and Xiao, Nong},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={39},
  number={10},
  pages={2588--2601},
  year={2019},
  publisher={IEEE}
}
</div>
                        <p>
                            <strong>
                                <a href="https://ieeexplore.ieee.org/document/8942126">A Deep-Reinforcement-Learning-Based Scheduler for FPGA HLS</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Minghua Shen<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        ICCAD<span class="tooltiptext">IEEE/ACM International Conference on Computer-Aided Design</span>
                                    </span>
                                </i>
                                , 2019
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('drl-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('drl-bib')">[bib]</a>
                            |

                            <a href="https://github.com/sysu-eda/DeepRL-Scheduling">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="drl-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">As the most critical stage in FPGA HLS, scheduling depends heavily on heuristics due to their speed, flexibility, and scalability. However, designing heuristics easily involves human bias, which makes scheduling unpredictable in some specific cases. To solve the problem, we propose an efficient deep reinforcement learning (Deep-RL) based scheduler for FPGA HLS. It has the potential to reduce the human involvement maximumly and learn to schedule by itself. The proposed scheduler consists of three steps. First, we design a novel state and action representation for constrained scheduling problems, which is the foundation of the learning task. Then, we leverage a training pipeline to train the policy network. Specifically, supervised learning is used to initialize the weights of the network and reinforcement learning is used to improve the performance, both of which make the Deep-RL based scheduler practical for HLS. At last, we compare our scheduler with the ASAP schedule and the optimal ILP schedule. Experimental results show that the proposed scheduler can reduce up to 74% resource usage compared with the original ASAP schedule, and the gap between the optimal solution is very small. Notably, this is the first work leveraging reinforcement learning in HLS and has great potential to be integrated into different HLS systems.
</div>
                        <div id="drl-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@inproceedings{chen2019deep,
  title={A deep-reinforcement-learning-based scheduler for fpga hls},
  author={Chen, Hongzheng and Shen, Minghua},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}
</div>
                        <p>
                            <span style="color: #666; font-size: 0.9em;">(* indicates equal contribution)</span>
                        </p>
                        <h4 id="workshops--preprints">Workshops / Preprints</h4>
                        <p>
                            <strong>
                                <a href="https://arxiv.org/abs/2509.06794">Dato: A Task-Based Programming Model for Dataflow Accelerators</a>
                            </strong>
                            <br/>
                            Shihan Fang*, <strong>Hongzheng Chen</strong>
                            *, Niansong Zhang, Jiajie Li, Han Meng, Adrian Liu, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>arXiv:2509.06794</i>
                                , 2025
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('dato-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('dato-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/allo">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="dato-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">Recent deep learning workloads increasingly push computational demand beyond what current memory systems can sustain, with many kernels stalling on data movement rather than computation. While modern dataflow accelerators incorporate on-chip streaming to mitigate off-chip bandwidth limitations, existing programming models struggle to harness these capabilities effectively. Low-level interfaces provide fine-grained control but impose significant development overhead, whereas high-level tile-based languages abstract away communication details, restricting optimization and forcing compilers to reconstruct the intended dataflow. We present Dato, a Python-embedded, task-based programming model for dataflow accelerators that elevates data communication and sharding to first-class type constructs. Developers write programs as a graph of tasks connected via explicit stream types, with sharded inputs specified using layout types. These tasks are first mapped virtually onto the accelerator's spatial fabric, and the compiler then generates a physical mapping that respects hardware constraints. Experimental results on both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves high performance while significantly reducing the burden of writing optimized code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and delivers a 2.81x speedup on attention kernels compared to a state-of-the-art commercial framework. On the FPGA, Dato surpasses leading frameworks in performance when generating custom systolic arrays, achieving 98% of the theoretical peak performance.
</div>
                        <div id="dato-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{fang2025dato,
    title={Dato: A Task-Based Programming Model for Dataflow Accelerators}, 
    author={Shihan Fang and Hongzheng Chen and Niansong Zhang and Jiajie Li and Han Meng and Adrian Liu and Zhiru Zhang},
    journal={arXiv preprint arXiv:2509.06794},
    year={2025}
}
</div>
                        <p>
                            <strong>
                                <a href="https://arxiv.org/abs/2506.07972">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            *, Yingheng Wang*, Yaohui Cai*, Hins Hu*, Jiajie Li*, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        Math-AI @ NeurIPS<span class="tooltiptext">The 5th Workshop on Mathematical Reasoning and AI at NeurIPS</span>
                                    </span>
                                </i>
                                , 2025
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('heurigym-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('heurigym-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/heurigym">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="heurigym-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.
</div>
                        <div id="heurigym-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2025heurigym,
    title={HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization}, 
    author={Hongzheng Chen and Yingheng Wang and Yaohui Cai and Hins Hu and Jiajie Li and Shirley Huang and Chenhui Deng and Rongjian Liang and Shufeng Kong and Haoxing Ren and Samitha Samaranayake and Carla P. Gomes and Zhiru Zhang},
    journal={arXiv preprint arXiv:2506.07972},
    year={2025}
}
</div>
                        <p>
                            <strong>
                                <a href="https://www.c4ml.org/c4ml-2025">Allo: Catalyzing Accelerator Design and Programming for Machine Learning</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            , Niansong Zhang, Shaojie Xiang, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        C4ML@CGO<span class="tooltiptext">Compilers for Machine Learning Workshop at International Symposium on Code Generation and Optimization</span>
                                    </span>
                                </i>
                                , 2025
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('allo-c4ml-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('allo-c4ml-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/allo">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="allo-c4ml-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">
                            As the benefits of technology scaling diminish, specialized hardware accelerators are crucial for performance in emerging machine learning applications. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. While new accelerator design languages (ADLs) aim to enhance or replace HLS, they are typically more effective for simple applications with a single kernel, rather than for hierarchical designs with multiple kernels.
<br/>
                            <br/>
                            In the first part of this talk, we will introduce Allo, a composable programming model for efficient hardware accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data types from algorithm specification, and encapsulates them as a set of customization primitives, which enables verifiable stepwise optimizations. Allo also preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner, enabling both temporal and spatial composition.
<br/>
                            <br/>We will then illustrate how Allo optimizes large-scale designs with two case studies. First, we develop a spatial accelerator architecture for large language models (LLMs) and prototype it on an AMD U280 FPGA, demonstrating higher energy efficiency than NVIDIA GPUs in generative inference settings. In addition, we deploy a convolutional neural network (CNN) design using Allo on the AMD Ryzen AI Engine, achieving substantial speedups over prior approaches.

                        </div>
                        <div id="allo-c4ml-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2024allo,
  title={Allo: A programming model for composable accelerator design},
  author={Chen, Hongzheng and Zhang, Niansong and Xiang, Shaojie and Zeng, Zhichen and Dai, Mengjia and Zhang, Zhiru},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={PLDI},
  pages={593--620},
  year={2024},
  publisher={ACM New York, NY, USA}
}
</div>
                        <p>
                            <strong>
                                <a href="files/papers/sch-pldi24-src.pdf">ü•â Uncovering Magic with Magic: Schedule Reconstruction from High-Performance Kernel Libraries</a>
                            </strong>
                            <br/>
                            <strong>Hongzheng Chen</strong>
                            <br/>
                            <span style="color:gray">
                                <i>
                                    <span class="tooltip">
                                        PLDI Student Research Competition (SRC)<span class="tooltiptext">ACM SIGPLAN Conference on Programming Language Design and Implementation Student Research Competition</span>
                                    </span>
                                </i>
                                , 2024 
                                <font color="brown">
                                    <b>(Bronze)</b>
                                </font>
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('src-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('src-bib')">[bib]</a>
                            |

                            <a href="https://github.com/cornell-zhang/allo">
                                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                            </a>
                        </p>
                        <div id="src-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">To fully harness the power of hardware accelerators, high-performance kernels are essential for various application domains in deep learning and scientific computing. Many of these kernels are carefully crafted and optimized by hand using languages like C/C++, demanding significant engineering efforts. Recent developments in kernel libraries have embraced schedule languages that decouple program optimization (ie, schedule) from algorithm specification. Users only need to write a few lines of schedule code to transform a basic program into an optimized one. However, this shift merely transfers the coding burden from writing optimized kernels to writing efficient schedules. While autoschedulers help generate optimized programs automatically, users have little control over the optimizations and their granularity remains opaque. To tackle this challenge, we introduce a novel research problem: schedule reconstruction, that is, reconstructing the schedule primitives from an optimized kernel implementation. Unlike existing approaches that focus on generating optimized kernels, our goal is to reveal the optimizations within these kernels by generating stepby-step schedule primitives. By providing an algorithm specification and an optimized kernel implementation, we propose a two-stage program synthesis technique to automatically generate the necessary schedule for this transformation. While this paper represents an initial exploration of schedule synthesis, our work demonstrates the potential of this idea to:(1) Aiding programmers in comprehending the intricacies of high-performance program optimizations, making it easier to debug and allowing them to experiment with different tradeoff combinations. (2) Potentially providing templates for automating compilers to generate even more high-performance code than manually optimized ones.
</div>
                        <div id="src-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{chen2024pldisrc,
  title={Uncovering Magic with Magic: Schedule Reconstruction from High-Performance Kernel Libraries},
  author={Chen, Hongzheng},
  journal={ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI) Student Research Competition (SRC)},
  year={2024}
}
</div>
                        <p>
                            <strong>
                                <a href="https://arxiv.org/abs/2203.02549">Structured Pruning is All You Need for Pruning CNNs at Initialization</a>
                            </strong>
                            <br/>
                            Yaohui Cai, Weizhe Hua, <strong>Hongzheng Chen</strong>
                            , G. Edward Suh, Christopher De Sa, Zhiru Zhang<br/>
                            <span style="color:gray">
                                <i>arXiv:2203.02549</i>
                                , 2022
                            </span>
                            |
<a href="javascript:void(0)" onclick="toggleAbstract('pai-abs')">[abs]</a>
                            |
<a href="javascript:void(0)" onclick="toggleBib('pai-bib')">[bib]</a>
                        </p>
                        <div id="pai-abs" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 0.9em;">Pruning is a popular technique for reducing the model size and computational cost of convolutional neural networks (CNNs). However, a slow retraining or fine-tuning procedure is often required to recover the accuracy loss caused by pruning. Recently, a new research direction on weight pruning, pruning-at-initialization (PAI), is proposed to directly prune CNNs before training so that fine-tuning or retraining can be avoided. While PAI has shown promising results in reducing the model size, existing approaches rely on fine-grained weight pruning which requires unstructured sparse matrix computation, making it difficult to achieve real speedup in practice unless the sparsity is very high. This work is the first to show that fine-grained weight pruning is in fact not necessary for PAI. Instead, the layerwise compression ratio is the main critical factor to determine the accuracy of a CNN model pruned at initialization. Based on this key observation, we propose PreCropping, a structured hardware-efficient model compression scheme. PreCropping directly compresses the model at the channel level following the layerwise compression ratio. Compared to weight pruning, the proposed scheme is regular and dense in both storage and computation without sacrificing accuracy. In addition, since PreCropping compresses CNNs at initialization, the computational and memory costs of CNNs are reduced for both training and inference on commodity hardware. We empirically demonstrate our approaches on several modern CNN architectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10 and ImageNet.
</div>
                        <div id="pai-bib" style="display:none; margin-top:10px; margin-bottom:10px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family: monospace; white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word; font-size: 0.85em;">@article{cai2022pai,
  title={Structured pruning is all you need for pruning CNNs at initialization},
  author={Cai, Yaohui and Hua, Weizhe and Chen, Hongzheng and Suh, G Edward and De Sa, Christopher and Zhang, Zhiru},
  journal={arXiv preprint arXiv:2203.02549},
  year={2022}
}
</div>
                        <!--
LLVM Developer Meeting'25, Santa Clara, CA
FDF'25, Geneva, Switzerland
University of Edinburgh, Edinburgh, UK
JSXC'25, New York, NY
CGO'25, Las Vegas, NV
FPGA'25, Monterey, CA
ACE Annual Review'24, Chicago, IL
MLSys Rising Stars, Santa Clara, CA
PLDI'24, Copenhagen, Denmark
CVPR'24, Seattle, WA
FCCM'24, Orlando, FL
ASPLOS'24, San Diego, CA
FPGA'24, Monterey, CA
TECHCON'23, Austin, TX
OSDI'23/ATC'23, Boston, MA
ISCA'23/PLDI'23, Orlando, FL
NSDI'23, Boston, MA
ASPLOS'23, Vancouver (virtual), Canada
MLSys'22, Santa Clara, CA
ISCA'22, New York, NY
FCCM'22, New York, NY
SC'21, St. Louis, MO
ICCAD'19, Denver, CO
-->
                        <h3 id="education">Education</h3>
                        <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="0" style="margin-top:0px; margin-bottom:10px;">
                            <tr>
                                <td width="50%" valign="top">
                                    <b>Cornell University</b>
                                    , US<br/>
                                    Ph.D. in Computer Science<br/>
                                </td>
                                <td width="38%" align="right" valign="top">Aug. 2021 - Present</td>
                                <td width="10%" rowspan="2" valign="top">
                                    <img src="/images/cu-logo.png" width="100%" style="display: flex; justify-content: center;"/>
                                </td>
                            </tr>
                            <tr>
                                <td colspan="2" valign="top">
                                    Thesis: 
                                    <a href="">
                                        <strong>Composable Programming Models for Accelerated Computing</strong>
                                    </a>
                                    <br/>
                                    Committee: <a href="https://www.csl.cornell.edu/~zhiruz/">Zhiru Zhang</a>
                                    , <a href="https://www.cs.cornell.edu/~asampson/">Adrian Sampson</a>
                                    , <a href="https://www.mohsaied.com/">Mohamed Abdelfattah</a>
                                    <br/>Accumulated GPA: 4.0/4.0
  
                                </td>
                            </tr>
                            <tr>
                                <td width="50%" valign="top">
                                    <b>Cornell University</b>
                                    , US<br/>
                                    M.S. in Computer Science<br/>
                                </td>
                                <td width="38%" align="right" valign="top">Aug. 2021 - Dec. 2024</td>
                                <td width="10%" rowspan="2" valign="top">
                                    <img src="/images/cu-logo.png" width="100%" style="display: flex; justify-content: center;"/>
                                </td>
                            </tr>
                            <tr></tr>
                            <tr>
                                <td width="50%" valign="top">
                                    <b>Sun Yat-sen University</b>
                                    , China<br/>B.E. in Computer Science
                                </td>
                                <td width="38%" align="right" valign="top">Aug. 2017 - Jun. 2021</td>
                                <td width="10%" rowspan="2" valign="top">
                                    <img src="/images/sysu-logo.png" width="100%" style="display: flex; justify-content: center;"/>
                                </td>
                            </tr>
                            <tr>
                                <td colspan="2" valign="top">
                                    <!-- Thesis: <a href=""><strong>High-Performance Concurrent Graph Processing System</strong></a><br/>
  <font color="red"><b>(Outstanding Undergraduate Thesis)</b></font><br/> -->
                                    Overall GPA: 3.95/4.00 (Major GPA: 3.99/4.00)<br/>Ranking: 1/188
  
                                </td>
                            </tr>
                        </table>
                        <h3 id="work-experience">Work Experience</h3>
                        <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="0" style="margin-top:0px; margin-bottom:10px;">
                            <tr>
                                <td width="70%" valign="top">
                                    <a href="https://about.google/">
                                        <b>Google DeepMind</b>
                                        &nbsp;
                                    </a>
                                    <img src="/images/google-logo.png" width="3%" style="vertical-align:middle;"/>
                                    , Sunnyvale, CA, US<br/>
                                    Student Researcher, Compiler Optimization Team<br/>
                                    Mentors: <a href="https://research.google/people/107560/">Mircea Trofin</a>
                                    and <a href="https://www.ayazdan.com/">Amir Yazdanbakhsh</a>
                                </td>
                                <td width="30%" align="right" valign="top">May 2025 - Dec. 2025</td>
                            </tr>
                            <tr>
                                <td width="75%" valign="top">
                                    <a href="https://www.nvidia.com/en-us/">
                                        <b>NVIDIA</b>
                                        &nbsp;
                                    </a>
                                    <img src="/images/nvidia-logo.png" width="3%" style="vertical-align:middle;"/>
                                    , Redmond, WA, US<br/>
                                    Machine Learning Compiler Research Intern, Deep Learning Compiler Technology Team<br/>
                                    Mentors: <a href="https://www.linkedin.com/in/fanbin">Bin Fan</a>
                                    and <a href="https://scholar.google.com/citations?user=DwTbLh4AAAAJ&amp;hl=en">Vinod Grover</a>
                                </td>
                                <td width="30%" align="right" valign="top">May 2024 - Nov. 2024</td>
                            </tr>
                            <tr>
                                <td width="70%" valign="top">
                                    <a href="https://aws.amazon.com/ai/">
                                        <b>Amazon Web Services (AWS)</b>
                                        &nbsp;
                                    </a>
                                    <img src="/images/aws-logo.png" width="5%" style="vertical-align:middle;"/>
                                    , Santa Clara, CA, US<br/>
                                    Applied Scientist Intern, Deep Engine-Science Team<br/>
                                    Mentors: <a href="https://comaniac.github.io/">Cody Hao Yu</a>
                                    , <a href="https://szhengac.github.io/">Shuai Zheng</a>
                                    , and <a href="http://yidawang.org/">Yida Wang</a>
                                </td>
                                <td width="30%" align="right" valign="top">Aug. 2022 - Apr. 2023</td>
                            </tr>
                            <tr>
                                <td width="70%" valign="top">
                                    <a href="https://www.bytedance.com/en/">
                                        <b>ByteDance AI Lab</b>
                                        &nbsp;
                                    </a>
                                    <img src="/images/bd-logo.jpg" width="4%" style="vertical-align:middle;"/>
                                    , Beijing, China<br/>
                                    Research Intern, MLSys Team, Applied Machine Learning (AML)<br/>
                                    Mentors: <a href="https://www.linkedin.com/in/jun-he-15a3263b/">Jun He</a>
                                    and <a href="http://yibozhu.com/">Yibo Zhu</a>
                                </td>
                                <td width="30%" align="right" valign="top">Aug. 2020 - May 2021</td>
                            </tr>
                        </table>
                        <h3 id="teaching">Teaching</h3>
                        <ul>
                            <li>
                                <p>
                                    <a href="https://cs3110.github.io/textbook/cover.html">
                                        <strong>CS3110: Data Structures and Functional Programming</strong>
                                    </a>
                                    <br/>Spring 2022, TA, Cornell Universiy
                                </p>
                            </li>
                            <li>
                                <p>
                                    <a href="https://www.cs.cornell.edu/~bracy/teach/">
                                        <strong>CS3410: Computer System Organization and Programming</strong>
                                    </a>
                                    <br/>Fall 2021, TA, Cornell Universiy
                                </p>
                            </li>
                        </ul>
                        <h3 id="professional-service">Professional Service</h3>
                        <ul>
                            <li>
                                <strong>Conference Reviewer:</strong>
                                <a href="https://mlsys.org/Conferences/2026">MLSys‚Äô26</a>
                                , <a href="https://iclr.cc/">ICLR‚Äô26</a>
                                , <a href="https://aaai.org/conference/aaai/aaai-26/">AAAI‚Äô26</a>
                                , <a href="https://neurips.cc/">NeurIPS‚Äô25</a>
                                , <a href="https://icml.cc/">ICML‚Äô25</a>
                                , <a href="https://mlsys.org/Conferences/2025/ProgramCommittee">MLSys‚Äô25</a>
                                , <a href="https://iclr.cc/Conferences/2025">ICLR‚Äô25</a>
                                , <a href="https://neurips.cc/">NeurIPS‚Äô24</a>
                                , <a href="https://mlsys.org/">MLSys‚Äô24</a>
                                , <a href="https://iccad.com/">ICCAD‚Äô22</a>
                            </li>
                            <li>
                                <strong>Journal Reviewer:</strong>
                                <ul>
                                    <li>
                                        <a href="https://www.computer.org/csdl/journal/tc">IEEE Transactions on Computers (TC)</a>
                                    </li>
                                    <li>
                                        <a href="https://ieee-ceda.org/publication/ieee-transactions-computer-aided-design-integrated-circuits-systems-tcad">IEEE Transactions on Computer Aided Design of Integrated Circuits &amp;Systems (TCAD)</a>
                                    </li>
                                    <li>
                                        <a href="https://dl.acm.org/journal/trets">ACM Transactions on Reconfigurable Technology and Systems (TRETS)</a>
                                    </li>
                                    <li>
                                        <a href="https://dl.acm.org/journal/todaes">ACM Transactions on Design Automation of Electronic Systems (TODAES)</a>
                                    </li>
                                    <li>
                                        <a href="https://ieee-cas.org/publication/JETCAS">IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS)</a>
                                    </li>
                                    <li>
                                        <a href="https://link.springer.com/journal/11227">The Journal of Supercomputing (JSC)</a>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Artifact Evaluation Committee:</strong>
                                <a href="https://www.usenix.org/conference/osdi25">OSDI‚Äô25</a>
                                , <a href="https://www.usenix.org/conference/atc25">ATC‚Äô25</a>
                                , <a href="https://2025.splashcon.org/track/OOPSLA">OOPSLA‚Äô25</a>
                                , <a href="https://www.usenix.org/conference/osdi24">OSDI‚Äô24</a>
                                , <a href="https://www.usenix.org/conference/atc24">ATC‚Äô24</a>
                                , <a href="https://conf.researchr.org/profile/conf/hongzhengchen">PLDI‚Äô24</a>
                                , <a href="https://2024.splashcon.org/">OOPSLA‚Äô24</a>
                                , <a href="https://sysartifacts.github.io/sosp2023/">SOSP‚Äô23</a>
                                , <a href="https://www.usenix.org/conference/osdi23/call-for-artifacts">OSDI‚Äô23</a>
                                , <a href="https://www.usenix.org/conference/osdi23/call-for-artifacts">ATC‚Äô23</a>
                                , <a href="https://mlsys.org/">MLSys‚Äô23</a>
                                , <a href="https://pldi23.sigplan.org/">PLDI‚Äô23</a>
                                , <a href="https://www.usenix.org/conference/osdi22/call-for-artifacts">OSDI‚Äô22</a>
                                , <a href="https://www.usenix.org/conference/atc22/call-for-artifacts">ATC‚Äô22</a>
                                , <a href="https://pldi22.sigplan.org/track/pldi-2022-PLDI-Research-Artifacts#About">PLDI‚Äô22</a>
                                , <a href="https://sysartifacts.github.io/eurosys2022/">EuroSys‚Äô22</a>
                                , <a href="https://sysartifacts.github.io/sosp2021/">SOSP‚Äô21</a>
                            </li>
                            <li>
                                <strong>Student Volunteer:</strong>
                                <a href="https://www.cs.cornell.edu/phd">Cornell CS PhD Application Committee‚Äô22-26</a>
                                , <a href="https://www.sigplan.org/LongTermMentoring/">SIGPLAN-M</a>
                                , <a href="https://www.isfpga.org/program/">FPGA‚Äô24</a>
                                , <a href="https://www.fccm.org/volunteer-2022/">FCCM‚Äô22</a>
                            </li>
                        </ul>
                        <h3 id="awards--honors">Awards &amp;Honors</h3>
                        <ul>
                            <li>
                                <a href="https://www.isfpga.org/">
                                    <strong>FPGA‚Äô25 Best Paper Nominee</strong>
                                </a>
                                , FPGA, 2025
                            </li>
                            <li>
                                <a href="https://pldi24.sigplan.org/track/pldi-2024-src">
                                    <strong>PLDI‚Äô24 Student Research Competition (SRC) 3rd Place</strong>
                                </a>
                                , SIGPLAN, 2024
                            </li>
                            <li>
                                <a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/">
                                    <strong>ML and Systems Rising Stars</strong>
                                </a>
                                , MLCommons, 2024
                            </li>
                            <li>
                                <a href="https://dl.acm.org/doi/10.1145/3626202.3637563">
                                    <strong>FPGA‚Äô24 Best Paper Award</strong>
                                </a>
                                , FPGA, 2024
                            </li>
                            <li>
                                <a href="https://www.isfpga.org/past/fpga2021/program/">
                                    <strong>FPGA‚Äô21 Best Paper Nominee</strong>
                                </a>
                                , FPGA, 2021
                            </li>
                            <li>
                                <a href="">
                                    <strong>Outstanding Undergraduate Thesis Award</strong>
                                </a>
                                , Sun Yat-sen University, 2021
                            </li>
                            <li>
                                <a href="https://www.ccf.org.cn/Membership/Individual_member/Honor/ccfxshytsjh/2022-04-13/760668.shtml">
                                    <strong>CCF Elite Collegiate Award</strong>
                                </a>
                                (98 undergrads in China), China Computer Federation (CCF), 2020
                            </li>
                            <li>
                                <a href="https://sites.google.com/view/ceda-hk/edathon-2019">
                                    <strong>IEEE EDAthon 2nd Place</strong>
                                </a>
                                , CEDA HK, 2019
                            </li>
                        </ul>
                        <h4 id="scholarship">Scholarship</h4>
                        <ul>
                            <li>
                                <strong>
                                    <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2025-north-america">Qualcomm Innovation Fellowship Finalist</a>
                                </strong>
                                , Qualcomm, 2025
                            </li>
                            <li>
                                <strong>
                                    <a href="https://www.sensetime.com/cn/scholarship-list">SenseTime Scholarship</a>
                                </strong>
                                (21 undergrads in China), SenseTime, 2020
                            </li>
                            <li>
                                <strong>Chinese National Scholarship $\times$ 2</strong>
                                (Top 1%), Ministry of Education of PRC, 2018-2020
                            </li>
                            <li>
                                <strong>First-Prize Scholarship $\times$ 3</strong>
                                (Top 5%), Sun Yat-sen University, 2017-2020
                            </li>
                            <li>
                                <strong>Samsung Scholarship</strong>
                                (Top 1%), Samsung Electronics, 2017-2018
                            </li>
                        </ul>
                        <h3 id="talks">Talks</h3>
                        <ul>
                            <li>
                                <strong>Automatic Warp Specialization for Modern GPUs with Asynchronous References</strong>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="">Meta AI Compiler Team</a>
                                            , Menlo Park, CA, Oct 29, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="">NVIDIA DL Compiler</a>
                                            , Redmond, WA, Aug 21, 2024
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</strong>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://llvm.swoogo.com/2025devmtg/agenda">LLVM Developers' Meeting</a>
                                            , Santa Clara, CA, Oct 28, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://discourse.llvm.org/t/llvm-dev25-llvm-ml-workshop/87488/4">LLVM‚ù§Ô∏èML Workshop @ LLVM Developers' Meeting</a>
                                            , Santa Clara, CA, Oct 27, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://research.google/teams/software-engineering-and-programming-languages/">Google ML Compiler Systems Research Team</a>
                                            , Online, Sep 25, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="">LLVM Summit</a>
                                            @ <a href="https://about.google/">Google</a>
                                            , Sunnyvale, CA, Aug 13, 2025
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</strong>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://idss.mit.edu/staff/cathy-wu/">Institute for Data, Systems, and Society (IDSS) @ MIT</a>
                                            , Online, Oct 8, 2025
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Dato: A Task-Based Programming Model for Dataflow Accelerators</strong>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://research.google/teams/software-engineering-and-programming-languages/">Google ML Compiler Systems Research Team</a>
                                            , Online, Sep 18, 2025
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Allo: A Programming Model for Composable Accelerator Design</strong>
                                <a href="files/talks/Allo-PLDI24-Jun28-2024.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <a href="https://www.youtube.com/live/WTKNLpemSDk?si=VNjaV-1xRP3Lq6DP&amp;t=15077">
                                    <i class="fa fa-video-camera" aria-hidden="true"></i>
                                </a>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">ACE Annual Review</a>
                                            , Chicago, IL, Oct 1, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">ACE Monthly Meeting</a>
                                            , Online, Aug 1, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://indico.cern.ch/event/1467417/">2nd FPGA Developers' Forum (FDF)</a>
                                            @ <a href="https://home.cern/">CERN</a>
                                            , Geneva, Switzerland, May 21, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://informatics.ed.ac.uk/icsa">ICSA</a>
                                            @ <a href="https://www.ed.ac.uk/">University of Edinburgh</a>
                                            , Edinburgh, UK, May 19, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.janestreet.com/">Jane Street Xcelerate Colloquium</a>
                                            , New York, May 16, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">ACE Spring Meeting</a>
                                            , Online, May 15, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://sites.coecis.cornell.edu/ececsl/home/csl-retreat-2025/">CSL Retreat @ Cornell Tech</a>
                                            , New York, NY, May 7, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://sharclab.ece.gatech.edu/teaching/2025-spring-fpga/">ECE 8893 ‚Äì Parallel Programming for FPGAs @ Georgia Tech</a>
                                            (Guest lecture), Online, Mar 11, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.c4ml.org/c4ml-2025">C4ML workshop @ CGO'25</a>
                                            , Las Vegas, NV, Mar 2, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.isfpga.org/">FPGA'25</a>
                                            , Monterey, CA, Mar 1, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://people.csail.mit.edu/mengjia/">CSAIL @ MIT</a>
                                            , Online, Feb 19, 2025
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://research.google/teams/software-engineering-and-programming-languages/">Google ML Compiler Systems Research Team</a>
                                            , Online, Dec 12, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">ACE Annual Review</a>
                                            , Chicago, IL, Oct 1, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://pldi24.sigplan.org/program/program-pldi-2024/">PLDI'24</a>
                                            , Copenhagen, Denmark, Jun 28, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="">NVIDIA DL Compiler</a>
                                            , Redmond, WA, May 29, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.fccm.org/call-for-demo-2024/">FCCM'24 Demo Night</a>
                                            , Orlando, FL, May 6, 2024
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Accelerating Large Language Model Inference on FPGA with Allo</strong>
                                <a href="files/talks/Allo-LLM-UW-May31-2024.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://sampl.cs.washington.edu/talks.html">UW SAMPL</a>
                                            , Seattle, WA, May 31, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://xilinx-center.csl.illinois.edu/">UIUC AMD-Xilinx Center of Excellence (HACC)</a>
                                            , Online, Apr 10, 2024
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference</strong>
                                <a href="files/talks/LLM-FPGA-FCCM24-May7-2024.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.fccm.org/technical-program-2024/">FCCM'24 </a>
                                            , Orlando, FL, May 7, 2024
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training</strong>
                                <a href="files/talks/Slapo-ASPLOS24-May1-2024.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <a href="https://www.youtube.com/watch?v=Fnw0ZJQToCY&amp;list=PLsLWHLZB96Vf7Y6HLu4SU0FNckl67GZTj&amp;index=177">
                                    <i class="fa fa-video-camera" aria-hidden="true"></i>
                                </a>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.bytedance.com/en/">ByteDance AML</a>
                                            , Bellevue, WA, Jun 14, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.asplos-conference.org/asplos2024/main-program/">ASPLOS'24</a>
                                            , San Diego, CA, May 1, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">ACE Liaison Meeting</a>
                                            , Online, Mar 5, 2024
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.src.org/calendar/e006829/">SRC TECHCON</a>
                                            , Austin, TX, Sep 12, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://charithmendis.com/team/">ADAPT Lab @ UIUC</a>
                                            , Online, Jul 17, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://acecenter.grainger.illinois.edu/">Spring ACE Center Meeting</a>
                                            , Orlando, FL, Jun 22, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.csl.cornell.edu/retreat/">CSL Retreat @ Cornell</a>
                                            , Ithaca, NY, May 12, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.bu.edu/cise/agenda-pre-nsdi-gathering-at-bu-april-23-2023/">Pre-NSDI Systems Gathering @ BU</a>
                                            , Boston, MA, Apr 16, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.amazon.jobs/en/teams/amazonai">Amazon AI</a>
                                            , Online, Apr 10, 2023
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://www.tvmcon.org/talks/decoupled-model-schedule-for-large-deep-learning-model-training">TVMCon</a>
                                            , Online, Mar 17, 2023
                                        </font>
                                        <!-- * <font color="gray"><a href="https://www.cs.cornell.edu/information/news/newsitem12657/researchers-share-work-opportunities-acsu-research-night">Cornell CIS Research Night</a>, Ithaca, Mar 13, 2023</font> -->
                                        <!-- * <font color="gray"><a href="https://www.ece.cornell.edu/ece">Cornell ECE Visit Day</a>, Ithaca, Mar 11, 2023</font> -->
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>An MLIR-Based Intermediate Representation for Accelerator Design with Decoupled Hardware Customizations</strong>
                                <a href="files/talks/HCL-MLIR-ODM-Aug11-2022.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <a href="https://youtu.be/yOk63LWbkqk">
                                    <i class="fa fa-video-camera" aria-hidden="true"></i>
                                </a>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://crispreasearch.blogspot.com/2022/09/an-mlir-based-intermediate.html">CRISP Liaison Meeting</a>
                                            , Online, Sep 28, 2022
                                        </font>
                                    </li>
                                    <li>
                                        <font color="gray">
                                            <a href="https://mlir.llvm.org/talks/">MLIR Open Design Meeting</a>
                                            , Online, Aug 11, 2022
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Krill: A Compiler and Runtime System for Concurrent Graph Processing</strong>
                                <a href="files/talks/Krill-SC21-Nov17-2021.pdf">
                                    <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
                                </a>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://sc21.supercomputing.org/presentation/?id=pap214&amp;sess=sess168">SC'21</a>
                                            , St. Louis, MO, Nov 17, 2021
                                        </font>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>A Deep-Reinforcement-Learning-Based Scheduler for FPGA HLS</strong>
                                <br/>
                                <ul>
                                    <li>
                                        <font color="gray">
                                            <a href="https://events-siteplex.confcats.io/iccad2022/wp-content/uploads/sites/72/2021/12/2019_ICCAD_ConferenceProgram_Web.pdf">ICCAD'19</a>
                                            , Denver, CO, Nov 5, 2019
                                        </font>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                        <script>
                            function toggleAbstract(id) {
                                var x = document.getElementById(id);
                                if (x.style.display === "none") {
                                    x.style.display = "block";
                                } else {
                                    x.style.display = "none";
                                }
                            }

                            function toggleBib(id) {
                                var x = document.getElementById(id);
                                if (x.style.display === "none") {
                                    x.style.display = "block";
                                } else {
                                    x.style.display = "none";
                                }
                            }
                        </script>
                        <style>
                            .tooltip {
                                position: relative;
                                display: inline-block;
                                cursor: help;
                                border-bottom: 1px dashed #666;
                            }

                            .tooltip .tooltiptext {
                                visibility: hidden;
                                width: 300px;
                                background-color: #555;
                                color: #fff;
                                text-align: center;
                                border-radius: 6px;
                                padding: 5px;
                                position: absolute;
                                z-index: 1;
                                bottom: 125%;
                                left: 50%;
                                margin-left: -150px;
                                opacity: 0;
                                transition: opacity 0.3s;
                            }

                            .tooltip:hover .tooltiptext {
                                visibility: visible;
                                opacity: 1;
                            }
                        </style>
                    </div>
                </article>
            </div>
        </main>
        <data class="u-url" href="/"></data>
        <footer class="site-footer h-card">
            <div class="wrapper">
                <div class="footer-col-wrapper">
                    <div class="footer-col one-half">
                        Updated in Nov. 2025<br/>
                        &copy;2019-2025 Hongzheng Chen.<br/>
                        Powered by <a href="http://jekyllrb.com">Jekyll</a>
                        &amp;<a href="https://pages.github.com/">Github Pages</a>
                        .
      
                    </div>
                    <div class="footer-col one-half">
                        <p style="text-align:right;">
                            <a href="https://yunmingzhang17.github.io/">Template</a>
                        </p>
                    </div>
                </div>
            </div>
        </footer>
    </body>
</html>
